version: '3.4'

x-logging-params: 
   &logging_params
   options:
      max-size: "1m"
      max-file: "10"
   driver: json-file
   
services:
   
  ####################### Proxy  ######################
  proxy:
    image: traefik:1.7.14
    restart: unless-stopped
    command: --web --docker --docker.exposedByDefault=false --loglevel=info
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./traefik/traefik.toml:/traefik.toml
      - ./traefik/cert:/ssl:ro
    ports:
      - "80:80"
      - "443:443"
    logging: *logging_params
      
  ######################################################

  ######################## CIMI  #######################
  cimi:
    image: mf2c/cimi-server:2.28
    restart: on-failure
    depends_on:
      - logicmodule1
      - dcproxy
    environment:
      - DC_HOST=dcproxy
      - DC_PORT=6472
      - EPHEMERAL_DB_BINDING_NS=com.sixsq.slipstream.db.dataclay.loader
      - PERSISTENT_DB_BINDING_NS=com.sixsq.slipstream.db.dataclay.loader
    expose:
      - "8201"
    labels:
      - "traefik.enable=true"
      - "traefik.backend=cimi"
      - "traefik.frontend.rule=PathPrefix:/api"
    volumes:
      - ringcontainer:/opt/slipstream/ring-container
      - ringcontainerexample:/opt/slipstream/ring-example
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD", "wget", "--spider", "http://localhost:8201/api/cloud-entry-point"]
    logging: *logging_params

  rc:
    image: sixsq/ring-container:3.52
    expose:
      - "5000"
    volumes:
      - ringcontainer:/opt/slipstream/ring-container
      - ringcontainerexample:/opt/slipstream/ring-example
    command: sh
    logging: *logging_params

  cimi-ui:
    image: mf2c/cimi-ui
    labels:
      - "traefik.enable=true"
      - "traefik.port=80"
      - "traefik.backend=cimi-ui"
      - "traefik.backend.healthcheck.path=/webui/webui.html"
      - "traefik.frontend.headers.customRequestHeaders=slipstream-authn-info:"
      - "traefik.frontend.rule=Path:/,/webui/,/{a:webui.*},/{b:css.*},/{c:images.*},/{d:js.*}"
    logging: *logging_params
      
  #################### Dataclay  #######################
  dcproxy:
    image: mf2c/dataclay-proxy:2.29
    depends_on:
      - logicmodule1
      - ds1java1
    expose:
      - "6472"
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD-SHELL", "health_check/wrapper_health_check.sh"]
    logging: *logging_params
       
  logicmodule1:
    image: mf2c/dataclay-logicmodule:2.29
    ports:
       - "1034:1034"
    environment:
      - DATACLAY_ADMIN_USER=admin
      - DATACLAY_ADMIN_PASSWORD=admin
      - LOGICMODULE_PORT_TCP=1034
      - LOGICMODULE_HOST=logicmodule1
    volumes:
      - ./prop/global.secure.properties:/usr/src/dataclay/cfgfiles/global.properties:ro
      - ./prop/log4j2.xml:/usr/src/dataclay/log4j2.xml:ro
    labels:
      - "traefik.enable=true"
      - "traefik.backend=logicmodule1"
      - "traefik.frontend.rule=Headers: service-alias,logicmodule1"
      - "traefik.port=1034"
      - "traefik.protocol=h2c"
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD-SHELL", "health_check/logicmodule_health_check.sh"]
    logging: *logging_params

  ds1java1:
    image: mf2c/dataclay-dsjava:2.29
    depends_on:
      - logicmodule1
    environment:
      - DATASERVICE_NAME=DS1
      - DATASERVICE_HOST=ds1java1
      - DATASERVICE_JAVA_PORT_TCP=2127
      - LOGICMODULE_PORT_TCP=1034
      - LOGICMODULE_HOST=logicmodule1
    volumes:
      - ./prop/global.properties:/usr/src/dataclay/cfgfiles/global.properties:ro
      - ./prop/log4j2.xml:/usr/src/dataclay/log4j2.xml:ro
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD-SHELL", "health_check/dataservice_health_check.sh"]
    logging: *logging_params
       
  #####################################################

  ################### Event Manager ###################
  event-manager:
    image: mf2c/cimi-server-events:1.0
    depends_on:
      - cimi
    expose:
      - 8000
    restart: unless-stopped
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD", "sh", "-c", "netstat -a | grep '0.0.0.0:8000'"]
    logging: *logging_params
      
  ######################################################

  ################## Service Manager ###################
  service-manager:
    image: mf2c/service-manager:1.10.1
    restart: unless-stopped
    depends_on:
      - cimi
    expose:
      - 46200
    labels:
      - "traefik.enable=true"
      - "traefik.frontend.rule=PathPrefixStrip:/sm"
      - "traefik.port=46200"
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD", "wget", "--spider", "http://localhost:46200/api"]
    logging: *logging_params
      
  ######################################################

  ########## Lifecycle Manager + User Manager ##########
  lm-um:
    image: mf2c/lifecycle-usermngt:1.3.9
    restart: on-failure
    depends_on:
      - cimi
      - resource-categorization
      - analytics-engine
      - landscaper
    expose:
      - "46300"
      - "46000"
    ports:
      - "46000:46000"
      - "46300:46300"
    environment:
      - WORKING_DIR_VOLUME=/tmp/mf2c/lm/compose_files
      - UM_WORKING_DIR_VOLUME=/tmp/mf2c/um/
      - LM_WORKING_DIR_VOLUME=/tmp/mf2c/lm/
      - SERVICE_CONSUMER=true
      - RESOURCE_CONTRIBUTOR=true
      - MAX_APPS=100
    volumes:
      - /tmp/mf2c/lm/compose_files:/tmp/mf2c/lm/compose_files
      - /tmp/mf2c/um:/tmp/mf2c/um
      - /tmp/mf2c/lm:/tmp/mf2c/lm
      - /var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      start_period: 60s
      retries: 5
      test: ["CMD", "wget", "--spider", "http://localhost:46000/api/v2/lm/isumalive"]
    logging: *logging_params
      
  ######################################################

  #################### SLA Manager #####################
  slalite:
    image: mf2c/sla-management:0.8.1
    restart: on-failure
    expose:
      - "46030"
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD", "wget", "--spider", "http://localhost:46030"]
    logging: *logging_params

  slalite-ui:
    image: mf2c/sla-ui:0.1.3
    restart: on-failure
    expose:
      - "8000"
    labels:
      - "traefik.enable=true"
      - "traefik.backend=slalite-ui"
      - "traefik.port=8000"
      - "traefik.frontend.rule=PathPrefix:/sla"
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD", "wget", "--spider", "http://localhost:8000/sla"]
    logging: *logging_params

  ######################################################

  ############ Resource Manager + Security #############
  policies:
     image: mf2c/policies:2.0.12
     restart: on-failure
     depends_on:
       - cimi
     ports:
       - "46050"
     labels:
       - "traefik.enable=true"
       - "traefik.frontend.rule=PathPrefix:/rm,/api/v2/resource-management/policies"
       - "traefik.port=46050"
     environment:
       - "DEBUG=True"
       - "MF2C=True"
       - "isLeader=${isLeader}"
       - "isCloud=${isCloud}"
       - "WIFI_DEV_FLAG=${WIFI_DEV_FLAG}"
       - "MF2C_CLOUD_AGENT=${MF2C_CLOUD_AGENT}"
     volumes:
       - vpninfo:/vpninfo
     healthcheck:
       start_period: 60s
       retries: 5
       test: ["CMD", "wget", "-O", "-", "-nv", "--content-on-error", "http://localhost:46050/api/v2/resource-management/policies/healthcheck"]
     logging: *logging_params

  discovery:
     image: mf2c/discovery:1.6
     restart: on-failure
     depends_on:
       - cimi
     network_mode: "host"
     ports:
     - "46040:46040"
     cap_add:
       - NET_ADMIN
     healthcheck:
       start_period: 30s
       retries: 5
       test: curl --fail -s http://localhost:46040/api/v1/resource-management/discovery/ || exit 1
     logging: *logging_params

  identification:
     image: mf2c/identification:v2
     restart: on-failure
     volumes:
       - mydata:/data
     expose:
       - 46060
     environment:
       - mF2C_User=${usr}
       - mF2C_Pass=${pwd}
     healthcheck:
       start_period: 30s
       retries: 5
       test: curl -f http://localhost:46060/api/v1/resource-management/identification/requestID || exit 1
     logging: *logging_params
       
  cau-client:
     image: mf2c/cau-client-it2:v2.1
     depends_on:
       - policies
     volumes:
       - pkidata:/pkidata
     expose:
       - 46065
     #using the cloud cau for the moment
     environment:
       - CCAU_URL=213.205.14.13:55443
       - CAU_URL=213.205.14.13:55443
     healthcheck:
       start_period: 60s
       interval: 60s
       timeout: 60s
       retries: 5
       test: ["CMD-SHELL", "echo getpubkey=1233211234567 | nc localhost 46065"]
     logging: *logging_params

  aclib:
     image: mf2c/aclib:v1.0
     depends_on:
       - cau-client
     volumes:
       - pkidata:/pkidata
     expose:
       - 46080
     environment:
       - ACLIB_PORT=46080
       - CAUCLIENT_PORT=46065
     healthcheck:
       start_period: 70s
       interval: 120s
       timeout: 60s
       retries: 3
       test: ["CMD-SHELL", "echo {\"sec\":\"pro\",\"comp\":\"f\",\"payload\":\"ping\",\"typ\":\"jws\"} | nc localhost 46080"]
     logging: *logging_params

  resource-categorization:
     image: mf2c/resource-categorization:resCatlatest-V2.0.30
     hostname: IRILD039
     restart: on-failure
     depends_on:
       - policies
     expose:
       - 46070
     environment:
       - "agentType=${agentType}"
       - "targetDeviceActuator=${targetDeviceActuator}"
       - "targetDeviceSensor=${targetDeviceSensor}"
     privileged: true
     volumes:
       - /var/run/docker.sock:/var/run/docker.sock
       - vpninfo:/vpninfo
       - /etc/hostname:/etc/hostname
     healthcheck:
       start_period: 60s
       retries: 5
       test: curl --fail -s http://localhost:46070/api/v1/resource-management/categorization/run/ || exit 1
     logging: *logging_params

  vpnclient:
    image: mf2c/vpnclient:1.1.5
    depends_on:
      - cau-client
    network_mode: "host"
    extra_hosts:
      - "vpnserver:213.205.14.13"
    environment:
      - "VPNINFO=/vpninfo/vpnclient.status"
    cap_add:
      - NET_ADMIN
    volumes:
      - pkidata:/pkidata
      - vpninfo:/vpninfo
    healthcheck:
      start_period: 120s
      interval: 30s
      test: grep -q connected /usr/share/nginx/html/api/get_vpn_ip
    logging: *logging_params

  resource-bootstrap:
    image: mf2c/resource-bootstrap:1.4.1
    restart: "no"
    environment:
      - CIMI_HOST=proxy
      - CIMI_PORT=80
      # mimics identification
      - MF2C_USER=${usr}
      - MF2C_PASS=${pwd}
    depends_on:
      - proxy
      - cimi
    logging: *logging_params

  ##### Analytics Engine + Recomender + Landscaper #####
  analytics-engine:
    image: mf2c/trunk:analytics-engine-0.6
    restart: on-failure
    expose:
      - "46020"
    ports:
      - "46020:46020"
    depends_on:
      - influxdb
    volumes:
      - ./analytics_engine.conf:/root/analytics_engine/analytics_engine.conf
    labels:
      - "traefik.enable=true"
      - "traefik.port=46020"
      - "traefik.frontend.rule=PathPrefixStrip:/analytics-engine"
    logging: *logging_params

  landscaper:
    image: mf2c/landscaper:0.6.14
    restart: on-failure
    volumes:
    - landscaper:/landscaper/data
    - ./landscaper.cfg:/landscaper/landscaper.cfg
    - ./ls_log:/landscaper/logs
    - /var/run/docker.sock:/var/run/docker.sock
    depends_on: ["cimi", "neo4j", "proxy"]
    environment:
      - OS_TENANT_NAME=
      - OS_PROJECT_NAME=
      - OS_TENANT_ID=
      - OS_USERNAME=
      - OS_PASSWORD=
      - OS_AUTH_URL=
    command: ["./start.sh", "http://neo4j:7474"]
    logging: *logging_params

  telem_start:
    image: mf2c/telem_start:0.1
    command: sh /download.sh
    environment:
      - HOSTNAME
    depends_on:
      - snap
    logging: *logging_params

  influxdb:
    image: influxdb
    expose:
      - "8086"
    ports:
      - "8086:8086"
    volumes:
      - influx:/var/lib/influxdb
    environment:
      - INFLUXDB_DB=snap
      - INFLUXDB_USER=snap
      - INFLUXDB_USER_PASSWORD=snap
      - INFLUXDB_ADMIN_USER=admin
      - INFLUXDB_ADMIN_USER_PASSWORD=admin
      - INFLUXDB_HTTP_LOG_ENABLED=false
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD", "wget", "--spider", "http://localhost:8086/ping"]
    logging: *logging_params
      
  snap:
    image: intelsdi/snap:xenial
    hostname: snap
    ports:
      - "8181:8181"
    volumes: ['/proc:/proc_host', '/sys/fs/cgroup:/sys/fs/cgroup', '/var/run/docker.sock:/var/run/docker.sock']
    depends_on:
      - influxdb
    environment:
      - SNAP_VERSION=2.0.0
      - SNAP_LOG_LEVEL=2
    logging: *logging_params

  neo4j:
    image: neo4j:2.3.12
    privileged: true
    volumes:
      - neo4j:/data
    ports:
      - "7475:7474" # expose the port for the console ui
    environment:
      - NEO4J_AUTH=neo4j/password # neo4j requires change from default password, should reflect landscape.cfg
    healthcheck:
      start_period: 1m
      retries: 5
      test: curl -i http://127.0.0.1:7474 2>&1 | grep -c -e '200 OK'
    logging: *logging_params

  web:
    image: mf2c/landscaper:0.6.14
    environment:
    - PYTHONPATH=/landscaper
    volumes:
      - ./landscaper.cfg:/landscaper/landscaper.cfg
      - ./ls_log:/collector_log
    ports:
      - "9001:9001"
    depends_on:
      - neo4j
      - landscaper
    command: ["/usr/local/bin/gunicorn", "-b", "0.0.0.0:9001", "-w", "2", "--chdir", "/landscaper/landscaper/web", "application:APP"]
    healthcheck:
      start_period: 90s
      retries: 5
      test: curl --fail -s http://localhost:9001/graph || exit 1
    logging: *logging_params


######################################################

volumes:
  mydata: {}
  influx: {}
  influx-analytics: {}
  neo4j: {}
  landscaper: {}
  ringcontainer: {}
  ringcontainerexample: {}
  pkidata: {}
  vpninfo: {}
